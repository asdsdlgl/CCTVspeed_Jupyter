{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Package<br>\n",
    "備註: 此程式會將frame數分為兩個model去訓練，並分別向左右延伸，找到最佳的訓練與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import Sequence, plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from models import *    #在jupyter中 改為 下面cell的 %run models.ipnyb\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "rn.seed(666)\n",
    "tf.set_random_seed(666)\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()           # 此三行的目的是讓TensorFlow在運行過程中動態申請顯存，需要多少就申請多少\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model<br>\n",
    "包括 c3d、c3g25、c4g12、c5g6、c6g3、lrcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Class Define<br>\n",
    "論文第10頁<br>\n",
    "參考 : 英文 https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly <br>\n",
    "      中文 https://blog.csdn.net/m0_37477175/article/details/79716312 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     53
    ]
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence): \n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs_1, list_IDs_3, labels, fnum, peak, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs_1 = list_IDs_1\n",
    "        self.list_IDs_3 = list_IDs_3\n",
    "        self.fnum = fnum\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.peak = peak\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs_1) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp_1, y = [self.list_IDs_1[k] for k in indexes],\\\n",
    "                             [self.labels[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X1 = self.__data_generation(list_IDs_temp_1, indexes)\n",
    "        return X1, np.array(y, dtype=int)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs_1))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(666)\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp_1, y_index):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X1 = []\n",
    "        # Generate data\n",
    "        for ID, y_ID in zip(list_IDs_temp_1, y_index):\n",
    "            # Store sample\n",
    "            x_1 = list(np.load(ID + '.npy'))\n",
    "            if len(x_1) >= 120:                       # 影片frame數 > 120 ，取70-119 frame\n",
    "                x_1 = x_1[70:120]\n",
    "            else:                                     # <120 則取 1-50 frame\n",
    "                x_1 = x_1[:frame_num]\n",
    "            while len(x_1) < self.fnum:               # <50  不夠的地方補0，像是如果(40,72,106,1)===>補0===>(50,72,106,1)\n",
    "                x_1.append(np.zeros_like(x_1[0]))\n",
    "            X1.append(x_1)\n",
    "        X1 = np.array(X1)                            # 因為batch設定32的關係  每次output為一(32,50,x,y,1)\n",
    "        return X1\n",
    "    \n",
    "class FrameInterval:\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.test_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.val_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "\n",
    "    def add_train(self, data, datatime):\n",
    "        self.train_df.loc[datatime] = [data]\n",
    "        self.train_df.index = pd.to_datetime(self.train_df.index)\n",
    "\n",
    "    def add_test(self, data, datatime):\n",
    "        self.test_df.loc[datatime] = [data]\n",
    "        self.test_df.index = pd.to_datetime(self.test_df.index)\n",
    "\n",
    "    def add_val(self, data, datatime):\n",
    "        self.val_df.loc[datatime] = [data]\n",
    "        self.val_df.index = pd.to_datetime(self.val_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define compare_mae<br>\n",
    "比較MAE(平均絕對誤差) |y-^y|/m <br>\n",
    "第一輪跑state = 0 所以先fit_generator 當等於state = 1時就predict_generator<br>\n",
    "state 0是加入testing data 跟 training data並且重新訓練<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compare_mae(new, train, test, frame_num, mae, model, stage, dir):\n",
    "    global state\n",
    "    print(\"Stage %d\" % stage, file=open(total_name, 'a'))\n",
    "    print(\"Current MAE:\", \"%.3f\" % mae, file=open(total_name, 'a'))\n",
    "    new_train = sorted(new + train)\n",
    "    new_test = sorted(new + test)\n",
    "    X_train = []\n",
    "    y_train_index = []\n",
    "    X_test = []\n",
    "    y_test_index = []\n",
    "    X_val = []\n",
    "    y_val_index = []\n",
    "    for t in new_test:\n",
    "        X_test += FrameList[t].test_df['data'].tolist() #Series to List\n",
    "        y_test_index += FrameList[t].test_df.index      #list of Timestamp\n",
    "        X_val += FrameList[t].val_df['data'].tolist()\n",
    "        y_val_index += FrameList[t].val_df.index\n",
    "    for t in new_train:\n",
    "        X_train += FrameList[t].train_df['data'].tolist()\n",
    "        y_train_index += FrameList[t].train_df.index\n",
    "    X_train_fnum = frame_df.loc[y_train_index]    #dataframe , select data of the days\n",
    "    X_test_fnum = frame_df.loc[y_test_index]\n",
    "    X_val_fnum = frame_df.loc[y_val_index]\n",
    "    y_train = vd_df.loc[y_train_index, feature].values\n",
    "    y_test = vd_df.loc[y_test_index, feature].values\n",
    "    y_val = vd_df.loc[y_val_index, feature].values\n",
    "    print(\"New Train Length:\", len(X_train), file=open(total_name, 'a'))\n",
    "    print(\"New Test Length:\", len(X_test), file=open(total_name, 'a'))\n",
    "    print(\"New Val Length:\", len(X_val), file=open(total_name, 'a'))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_train_fnum = np.array(X_train_fnum)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    X_test_fnum = np.array(X_test_fnum)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    X_val_fnum = np.array(X_val_fnum)\n",
    "    y_val = np.array(y_val)\n",
    "    testing_generator = DataGenerator(X_test, X_test_fnum, y_test, frame_num, dir, False) #將資料分批 IO較快\n",
    "\n",
    "    mae_2, mae_3 = 100, 100\n",
    "    if stage:     #後 執行  # 只有測試集延伸\n",
    "        testPrdct = model.predict_generator(generator=testing_generator, max_queue_size=500, workers=32, verbose=1)\n",
    "        testPrdct = np.squeeze(testPrdct)\n",
    "        mae_2 = abs(y_test[:len(testPrdct)] - testPrdct).mean()\n",
    "        print(\"Merge Test MAE:\", \"%.3f\" % mae_2, file=open(total_name, 'a'))\n",
    "\n",
    "        plt.figure(num=None, figsize=(50, 5))\n",
    "        plt.tick_params(labelsize=30)\n",
    "        plt.plot(y_test, lw=2, label='Target')\n",
    "        plt.plot(testPrdct, lw=2, label='Predict')\n",
    "        plt.legend(loc=1, prop={'size': 20})\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        # plt.title(mae_2)\n",
    "        plt.savefig(\n",
    "            \"plot/\" + cctv_Id + \"/\" + dir + \"/stage_%d_merge1.png\" % stage)\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "    if state == 0:  #先被執行 python中允許fit跟predict在不同次的function call # 包括測試集 驗證集 訓練集 都延伸 \n",
    "        training_generator = DataGenerator(X_train, X_train_fnum, y_train, frame_num, dir, True)\n",
    "        validation_generator = DataGenerator(X_val, X_val_fnum, y_val, frame_num, dir, True)\n",
    "        model_temp = model\n",
    "        history = model_temp.fit_generator(generator=training_generator, validation_data=validation_generator,\n",
    "                                           epochs=epochs, steps_per_epoch=len(X_train) // batch_size,\n",
    "                                           validation_steps=len(X_val) // batch_size,\n",
    "                                           max_queue_size=500, verbose=1, callbacks=[early_stopping], workers=32)\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=1)\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        plt.savefig(\n",
    "            \"plot/\" + cctv_Id + \"/\" + dir + \"/loss.png\")\n",
    "        testPrdct_2 = model_temp.predict_generator(generator=testing_generator, max_queue_size=500, workers=32, verbose=1)\n",
    "        testPrdct_2 = np.squeeze(testPrdct_2)\n",
    "        mae_3 = abs(y_test[:len(testPrdct_2)] - testPrdct_2).mean()\n",
    "        print(\"Merge All MAE:\", \"%.3f\" % mae_3, file=open(total_name, 'a'))\n",
    "        plt.figure(num=None, figsize=(50, 5))\n",
    "        plt.tick_params(labelsize=30)\n",
    "        plt.plot(y_test, lw=2, label='Target')\n",
    "        plt.plot(testPrdct_2, lw=2, label='Predict')\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        # plt.title(\"MAE: %.3f\" % mae_3)\n",
    "        plt.legend(loc=1, prop={'size': 20})\n",
    "        plt.savefig(\n",
    "            \"plot/\" + cctv_Id + \"/\" + dir + \"/stage_%d_merge2.png\" % stage)\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "    MAE = np.array([mae, mae_2, mae_3])\n",
    "    mini = MAE.argmin()\n",
    "    if stage == 0:\n",
    "        state = 0\n",
    "        print(\"State: Initial\", file=open(total_name, 'a'))\n",
    "        return new_train, new_test, mae_3, model_temp, False, testPrdct_2\n",
    "    if mini == 0:           #mini = 0 代表原本的是最準確的 所以就停止merging\n",
    "        state = 0\n",
    "        print(\"State: Stop merging\", file=open(total_name, 'a'))\n",
    "        return train, test, mae, model, True, None\n",
    "    elif mini == 1:           # 如果 e2 < e3 代表新加入訓練集的結果並不會比較好，讓state = 1 從此不加入新的測試集\n",
    "        state = 1\n",
    "        print(\"State: Add test -> %d\" % new[0], file=open(total_name, 'a'))\n",
    "        return train, new_test, mae_2, model, False, testPrdct\n",
    "    else:\n",
    "        state = 0\n",
    "        print(\"State: Add train & test -> %d\" % new[0], file=open(total_name, 'a'))\n",
    "        return new_train, new_test, mae_3, model_temp, False, testPrdct_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basic setting<br>\n",
    "day1, train_dayLast: 訓練時間<br>\n",
    "test_day1, dayLast: 測試時間<br>\n",
    "val_day1, val_dayLast: 驗證時間<br>\n",
    "feature: 選擇真值(Spd_Max, Spd_Min, Spd_Avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# vds = ['nfbVD-N5-N-16.900-M-PS', 'nfbVD-N5-N-18.313-M-PS', 'nfbVD-N5-N-22.510-M-PS', 'nfbVD-N5-N-25.310-M-PS',\n",
    "#        'nfbVD-N5-S-18.312-M-PS', 'nfbVD-N5-S-19.677-M-PS', 'nfbVD-N5-S-21.063-M-PS', 'nfbVD-N5-S-23.910-M-PS']\n",
    "\n",
    "# cctvs = ['nfbCCTV-N5-N-16.915-M', 'nfbCCTV-N5-N-18.308-M', 'nfbCCTV-N5-N-22.514-M', 'nfbCCTV-N5-N-25.309-M',\n",
    "#          'nfbCCTV-N5-S-18.339-M', 'nfbCCTV-N5-S-19.7-M', 'nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "vds = [\"nfbVD-N5-S-21.063-M-PS\", \"nfbVD-N5-S-23.910-M-PS\", \"nfbVD-N5-N-25.310-M-PS\"]\n",
    "cctvs = ['nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M', 'nfbCCTV-N5-N-25.309-M']\n",
    "\n",
    "features = ['Spd_Max', 'Spd_Avg', 'Spd_Min']\n",
    "\n",
    "day1 = datetime.strptime('20190728 0600', \"%Y%m%d %H%M\")\n",
    "day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "train_dayLast = datetime.strptime('20190729 2359', \"%Y%m%d %H%M\")\n",
    "\n",
    "test_day1 = datetime.strptime('20190728 0600', \"%Y%m%d %H%M\")\n",
    "test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "dayLast = datetime.strptime('20190729 2359', \"%Y%m%d %H%M\")\n",
    "dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "\n",
    "val_day1 = datetime.strptime('20190729 0600', \"%Y%m%d %H%M\")\n",
    "val_dayLast = val_day1 + timedelta(days=6) - timedelta(minutes=1)\n",
    "\n",
    "# day1 = datetime.strptime('20190320 0600', \"%Y%m%d %H%M\")\n",
    "# day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "# train_dayLast = datetime.strptime('20190320 0630', \"%Y%m%d %H%M\")\n",
    "#\n",
    "# test_day1 = datetime.strptime('20190328 0600', \"%Y%m%d %H%M\")\n",
    "# test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "# dayLast = datetime.strptime('20190328 0630', \"%Y%m%d %H%M\")\n",
    "# dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "# Add VDs\n",
    "# dt_dtypes = {'Spd0': int, 'Spd1': int, 'Spd2': int, 'SpdMax': int, 'SpdMin': int, 'SpdAvg': int}\n",
    "# train_dt_index = pd.date_range(day1, periods=(train_dayLast - day1).total_seconds() // 60 + 1, freq='T')\n",
    "# test_dt_index = pd.date_range(test_dayFirst, periods=(dayLast - test_dayFirst).total_seconds() // 60 + 1, freq='T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo<br>\n",
    "model: 要訓練的神經網路模型<br>\n",
    "filename: 選擇vd真值的csv檔<br>\n",
    "備註: 此程式會將frame數分為兩個model去訓練，並分別向左右延伸，找到最佳的訓練與測試集<br>\n",
    "frame_num : 取50frame 論文第10頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     32,
     36,
     39,
     48,
     61,
     74,
     98
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50, 72, 106, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 50, 36, 53, 32)    896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 36, 53, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 50, 18, 26, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 18, 26, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 25, 9, 13, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 25, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 25, 3744)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 25, 128)           1487232   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 25, 128)           98688     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,741,377\n",
      "Trainable params: 1,741,185\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "nfbCCTV-N5-S-21.048-M\n",
      "C3G25\n",
      "DatetimeIndex(['2019-07-29 16:41:00', '2019-07-29 16:42:00',\n",
      "               '2019-07-29 16:43:00', '2019-07-29 16:44:00',\n",
      "               '2019-07-29 16:45:00', '2019-07-29 16:46:00',\n",
      "               '2019-07-29 16:47:00', '2019-07-29 16:48:00',\n",
      "               '2019-07-29 16:49:00', '2019-07-29 16:50:00',\n",
      "               ...\n",
      "               '2019-07-30 16:32:00', '2019-07-30 16:33:00',\n",
      "               '2019-07-30 16:34:00', '2019-07-30 16:35:00',\n",
      "               '2019-07-30 16:36:00', '2019-07-30 16:37:00',\n",
      "               '2019-07-30 16:38:00', '2019-07-30 16:39:00',\n",
      "               '2019-07-30 16:40:00', '2019-07-30 16:41:00'],\n",
      "              dtype='datetime64[ns]', name=0, length=1081, freq=None)\n",
      "2019-08-26 01:56:45.095079\n",
      "Frame Num: 1081\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 141s 70s/step - loss: 67.8066 - acc: 0.0000e+00 - val_loss: 67.5499 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 67.4617 - acc: 0.0000e+00 - val_loss: 66.8417 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 27s 14s/step - loss: 66.7054 - acc: 0.0000e+00 - val_loss: 65.8361 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 65.7076 - acc: 0.0000e+00 - val_loss: 64.6099 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 64.5195 - acc: 0.0000e+00 - val_loss: 63.2408 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 63.1802 - acc: 0.0000e+00 - val_loss: 61.7710 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 61.8439 - acc: 0.0000e+00 - val_loss: 60.2344 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 60.2768 - acc: 0.0000e+00 - val_loss: 58.6568 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 58.7982 - acc: 0.0000e+00 - val_loss: 57.0516 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 57.2111 - acc: 0.0000e+00 - val_loss: 55.4257 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 55.6027 - acc: 0.0000e+00 - val_loss: 53.7787 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 53.9505 - acc: 0.0000e+00 - val_loss: 52.1201 - val_acc: 0.0156\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 52.2672 - acc: 0.0156 - val_loss: 50.4645 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 50.6205 - acc: 0.0000e+00 - val_loss: 48.7972 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 32s 16s/step - loss: 48.9275 - acc: 0.0156 - val_loss: 47.1057 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 27s 13s/step - loss: 47.2350 - acc: 0.0000e+00 - val_loss: 45.3717 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 27s 13s/step - loss: 45.5372 - acc: 0.0000e+00 - val_loss: 43.5981 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 43.7543 - acc: 0.0156 - val_loss: 41.8325 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 41.9741 - acc: 0.0000e+00 - val_loss: 40.0517 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 40.0737 - acc: 0.0000e+00 - val_loss: 38.2423 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 38.3199 - acc: 0.0156 - val_loss: 36.4521 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 36.5338 - acc: 0.0000e+00 - val_loss: 34.6591 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 34.7022 - acc: 0.0156 - val_loss: 33.0065 - val_acc: 0.0156\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 33.0856 - acc: 0.0000e+00 - val_loss: 31.5081 - val_acc: 0.0625\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 31.5371 - acc: 0.0312 - val_loss: 30.2759 - val_acc: 0.0156\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 30.2194 - acc: 0.0156 - val_loss: 29.1680 - val_acc: 0.0156\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 29.2192 - acc: 0.0156 - val_loss: 28.1653 - val_acc: 0.0625\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 28.1389 - acc: 0.0625 - val_loss: 27.4645 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 27.4845 - acc: 0.0000e+00 - val_loss: 26.7976 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 26.7456 - acc: 0.0000e+00 - val_loss: 26.1321 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 26.1183 - acc: 0.0000e+00 - val_loss: 25.4654 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 25.4104 - acc: 0.0000e+00 - val_loss: 24.7999 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 24.8025 - acc: 0.0000e+00 - val_loss: 24.1319 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 24.0689 - acc: 0.0000e+00 - val_loss: 23.4631 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 27s 14s/step - loss: 23.3603 - acc: 0.0000e+00 - val_loss: 22.7923 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 27s 13s/step - loss: 22.7329 - acc: 0.0000e+00 - val_loss: 22.1566 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 27s 14s/step - loss: 22.1294 - acc: 0.0000e+00 - val_loss: 21.5490 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 21.4976 - acc: 0.0000e+00 - val_loss: 20.9576 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 20.9388 - acc: 0.0000e+00 - val_loss: 20.4394 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 20.3814 - acc: 0.0000e+00 - val_loss: 19.9695 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 19.9106 - acc: 0.0000e+00 - val_loss: 19.5007 - val_acc: 0.0156\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 19.4789 - acc: 0.0000e+00 - val_loss: 19.0987 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 25s 13s/step - loss: 19.0663 - acc: 0.0000e+00 - val_loss: 18.7380 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 18.6688 - acc: 0.0000e+00 - val_loss: 18.4562 - val_acc: 0.0625\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 26s 13s/step - loss: 18.4049 - acc: 0.0625 - val_loss: 18.5855 - val_acc: 0.0156\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 27s 13s/step - loss: 18.5844 - acc: 0.0000e+00 - val_loss: 18.7609 - val_acc: 0.0312\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 18.7070 - acc: 0.0312 - val_loss: 18.8595 - val_acc: 0.0781\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 28s 14s/step - loss: 18.8121 - acc: 0.0625 - val_loss: 18.8613 - val_acc: 0.0781\n",
      "Epoch 49/100\n"
     ]
    }
   ],
   "source": [
    "# Todo: Tune the fitting parameters\n",
    "feature = 'Spd_Max'\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='auto',\n",
    "                               verbose=0, restore_best_weights=True)\n",
    "\"\"\"Choose model\"\"\"\n",
    "name, model = c3g25()\n",
    "\n",
    "total_name = 'plot/All_log.txt'\n",
    "open(total_name, 'w').close()\n",
    "\n",
    "for cctv_Id, vd_Id in zip(cctvs, vds):\n",
    "    print(cctv_Id)\n",
    "    print(name)\n",
    "    print(cctv_Id, file=open(total_name, 'a'))\n",
    "    vdDir = 'D:/vd/' + vd_Id + '/vd_speed/'\n",
    "    cctvDir = 'D:/wcs/cctv/' + cctv_Id + \"/\"\n",
    "    filename = vdDir + 'vdspd_20190728_20190729.csv'   #'vdspd_20190322_20190608.csv'\n",
    "    total_frames = cctvDir + \"frame_num.txt\"\n",
    "    vd_df = pd.read_csv(\"D:/vd/nfbVD-N5-N-25.310-M-PS/vd_speed/vdspd_20190728_20190729.csv\", infer_datetime_format=True, index_col=0)\n",
    "    vd_df.index = pd.to_datetime(vd_df.index)\n",
    "    vd_df = vd_df.shift(-1)                            #往上位移一位 因為資料取得有延遲\n",
    "    vd_df.loc[vd_df.index[len(vd_df)-1]] = 90          #將位移完後的nan填補90\n",
    "    frame_df = pd.read_csv(total_frames, infer_datetime_format=True, index_col=0, header=None)\n",
    "    frame_df.index = pd.to_datetime(frame_df.index)   #必須要用to_datetime 因為loc才能找到 從2019/07/29 ---> 2019-07-29\n",
    "    print(frame_df.index)\n",
    "    total_test_index = []\n",
    "    FrameList = []\n",
    "    with open(cctvDir + \"frame_interval.txt\") as f:\n",
    "        content = f.readlines()                                       #['26\\n','9\\n'.....]\n",
    "    frame_interval = np.array([x.strip() for x in content], dtype=int)\n",
    "    for i in range(len(content)):\n",
    "        FrameList.append(FrameInterval())\n",
    "    print(datetime.now())\n",
    "\n",
    "    if not os.path.exists(\"plot/\" + cctv_Id + \"/peak_1\"):\n",
    "        os.makedirs(\"plot/\" + cctv_Id + \"/peak_1\")\n",
    "        os.makedirs(\"plot/\" + cctv_Id + \"/peak_2\")\n",
    "    else:\n",
    "        for f in os.listdir(\"plot/\" + cctv_Id + \"/peak_1\"):\n",
    "            os.remove(\"plot/\" + cctv_Id + \"/peak_1/\" + f)\n",
    "        for f in os.listdir(\"plot/\" + cctv_Id + \"/peak_2\"):\n",
    "            os.remove(\"plot/\" + cctv_Id + \"/peak_2/\" + f)\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    val_count = 0\n",
    "    day = day1                       #first day of training day\n",
    "    while day <= train_dayLast:\n",
    "        # print(day)\n",
    "        if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\")\\\n",
    "                or day not in vd_df.index:\n",
    "            day += timedelta(minutes=1)\n",
    "            continue\n",
    "        train_count += 1\n",
    "        FrameList[int(frame_df.loc[day]) // 10]\\\n",
    "            .add_train(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\"), day)\n",
    "          #     \"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644\"      ,      \"2019-07-29 16:44:00\"\n",
    "        day += timedelta(minutes=1)\n",
    "\n",
    "    day = test_day1                 #first day of testing day\n",
    "    while day <= dayLast:\n",
    "        # print(day)\n",
    "        if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                or day not in vd_df.index:\n",
    "            day += timedelta(minutes=1)\n",
    "            continue\n",
    "        test_count += 1\n",
    "        FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "            .add_test(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\"), day)\n",
    "        total_test_index.append(day)\n",
    "        day += timedelta(minutes=1)\n",
    "\n",
    "    day = val_day1\n",
    "    while day <= val_dayLast:       #first day of validating day\n",
    "        # print(day)\n",
    "        if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                or day not in vd_df.index:\n",
    "            day += timedelta(minutes=1)\n",
    "            continue\n",
    "        val_count += 1\n",
    "        FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "            .add_val(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\"), day)\n",
    "        day += timedelta(minutes=1)\n",
    "\n",
    "    print(\"Total Train:\", train_count, file=open(total_name, 'a'))\n",
    "    print(\"Total test:\", test_count, file=open(total_name, 'a'))\n",
    "    print(\"Total val:\", val_count, file=open(total_name, 'a'))\n",
    "    print(\"Frame Num:\", len(frame_df))                                        # 取shape的(x,y,channel)不包含數量 shape[0]是數量\n",
    "    shape = np.load(FrameList[5].train_df.iloc[0].values[0]+'.npy').shape[1:] #學長好像沒用到? shape(59,72,106,1)\n",
    "    \n",
    "    #shape2 = np.load(FrameList[5].train_df.iloc[0].values[0]+'_timestamp.npy').shape[1:]\n",
    "    # my_models = [build_model(shape), build_model_2(shape), build_model_3(shape)]\n",
    "\n",
    "    ModelTrain_1 = [list(frame_interval).index(sorted(frame_interval)[-1])] #找出frame數最大的index\n",
    "    ModelTrain_1 = [ModelTrain_1[0] - 1] + ModelTrain_1 #向左找 備註 : 就是frame.png的最高峰(詳細看論文)\n",
    "    ModelTest_1 = ModelTrain_1.copy()\n",
    "\n",
    "    if abs(list(frame_interval).index(sorted(frame_interval)[-2]) -\n",
    "           list(frame_interval).index(sorted(frame_interval)[-1])) < 5:\n",
    "        ModelTrain_2 = [list(frame_interval).index(sorted(frame_interval)[-3])] #如果最高峰跟第二高峰相差5個index內\n",
    "    else:                                                                       #就選擇第三高峰 否則選擇第二高峰的index\n",
    "        ModelTrain_2 = [list(frame_interval).index(sorted(frame_interval)[-2])]\n",
    "    ModelTrain_2 = [ModelTrain_2[0] - 1] + ModelTrain_2                        #同時向左找一個index\n",
    "    ModelTest_2 = ModelTrain_2.copy()\n",
    "\n",
    "    train = [ModelTrain_1, ModelTrain_2]    #[ [最高峰index 最高峰index-1(向左找)],[最二峰index 最二峰index-1(向左找)] ]\n",
    "    test = [ModelTest_1, ModelTest_2]\n",
    "\n",
    "    total_test = []\n",
    "\n",
    "    for peaks in range(2):\n",
    "        state = 0\n",
    "        Index = train[peaks][0] - 1\n",
    "        frame_num = train[peaks][-1]*10 if train[peaks][-1]*10 < 90 else 90\n",
    "        frame_num = 50\n",
    "        my_model = model\n",
    "        print(\"\\n*Peak %d\" % (peaks+1), file=open(total_name, 'a'))\n",
    "        train[peaks], test[peaks], currentMAE, my_model, isStopMerging, predict =\\\n",
    "            compare_mae([], train[peaks], test[peaks], frame_num, 100, my_model, 0, \"peak_%d\" % (peaks+1))\n",
    "        # file_name = \"plot/\" + cctv_Id + \"/\" + test_day1_str + '_' + dayLast_str + '/log.txt'\n",
    "        # open(file_name, 'w').close()\n",
    "        stages = 0\n",
    "        print(\"Frame Num:\", frame_num, file=open(total_name, 'a'))\n",
    "        print(\"Left Hand Side\", file=open(total_name, 'a'))\n",
    "        while Index >= 0 and not isStopMerging:                      #index 從最高峰往左找\n",
    "            stages += 1\n",
    "            print(Index)\n",
    "            print(\"Index:\", Index, file=open(total_name, 'a'))\n",
    "            if len(FrameList[Index].train_df) > 60 and len(FrameList[Index].val_df) > 0 \\\n",
    "                    and len(FrameList[Index].test_df) > 0:\n",
    "                train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                    compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages, \"peak_%d\" % (peaks+1))\n",
    "                if p is not None:\n",
    "                    predict = p\n",
    "            else:\n",
    "                if len(FrameList[Index].test_df) > 0 and len(FrameList[Index].val_df) > 0:\n",
    "                    state = 1   #僅僅是丟進testing data set中\n",
    "                    train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                        compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages,\n",
    "                                    \"peak_%d\" % (peaks + 1))\n",
    "                    if p is not None:\n",
    "                        predict = p\n",
    "                else:\n",
    "                    print(\"No data!\", file=open(total_name, 'a'))\n",
    "                    print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"\", file=open(total_name, 'a'))\n",
    "                    break\n",
    "            print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "            print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "            print(\"\", file=open(total_name, 'a'))\n",
    "            Index -= 1\n",
    "        state = 0\n",
    "        isStopMerging = False\n",
    "        print(\"Right Hand Side\", file=open(total_name, 'a'))\n",
    "        Index = train[peaks][-1] + 1                               #index 從最高峰往右找\n",
    "        while Index < 21 and not isStopMerging:\n",
    "            stages += 1\n",
    "            print(Index)\n",
    "            print(\"Index:\", Index, file=open(total_name, 'a'))\n",
    "            if len(FrameList[Index].train_df) > 60 and len(FrameList[Index].val_df) > 0 \\\n",
    "                    and len(FrameList[Index].test_df) > 0:\n",
    "                train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                    compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages, \"peak_%d\" % (peaks+1))\n",
    "                if p is not None:\n",
    "                    predict = p\n",
    "            else:\n",
    "                if len(FrameList[Index].test_df) > 0 and len(FrameList[Index].val_df) > 0:\n",
    "                    state = 1\n",
    "                    train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                        compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages,\n",
    "                                    \"peak_%d\" % (peaks + 1))\n",
    "                    if p is not None:\n",
    "                        predict = p\n",
    "                else:\n",
    "                    print(\"No data!\", file=open(total_name, 'a'))\n",
    "                    print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"\", file=open(total_name, 'a'))\n",
    "                    break\n",
    "            print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "            print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "            print(\"\", file=open(total_name, 'a'))\n",
    "            Index += 1\n",
    "        total_test += test[peaks]           #找出最好的testing set\n",
    "        index = 0\n",
    "        for t in test[peaks]:\n",
    "            if len(FrameList[t].test_df) == 0:\n",
    "                continue\n",
    "            temp = predict[index:index+len(FrameList[t].test_df)].tolist()\n",
    "            while len(temp) < len(FrameList[t].test_df):\n",
    "                temp.append(temp[-1])\n",
    "            index += len(FrameList[t].test_df)\n",
    "            FrameList[t].test_df.insert(loc=0, column='predict', value=temp)\n",
    "        print(\"------------------\", file=open(total_name, 'a'))\n",
    "        my_model.save(\"plot/\" + cctv_Id + \"/peak_%d\" % (peaks+1) + \"/model.h5\")\n",
    "    predict_df = pd.DataFrame(columns=['value'])\n",
    "    for i in total_test_index:\n",
    "        for j in total_test:\n",
    "            if i in FrameList[j].test_df.index:\n",
    "                predict_df.loc[i] = [FrameList[j].test_df.loc[i]['predict']]\n",
    "    df = pd.DataFrame(index=total_test_index)\n",
    "    predict_all = np.array(predict_df.loc[total_test_index, 'value'].values)\n",
    "    vd_all = np.array(vd_df.loc[total_test_index, feature].values)\n",
    "    df.insert(loc=0, column='target', value=vd_all)\n",
    "    df.insert(loc=0, column='predict', value=predict_df)\n",
    "    df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='ffill')\n",
    "    df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='bfill')\n",
    "    df.to_csv(\"plot/\" + cctv_Id + \"/total_result.csv\")\n",
    "    print(\"Total predict:\", len(predict_df), file=open(total_name, 'a'))\n",
    "    print(\"Total test:\", len(vd_all), file=open(total_name, 'a'))\n",
    "    y = np.array(df['target'].values)\n",
    "    p = np.array(df['predict'].values)\n",
    "    mae = abs(y - p).mean()\n",
    "    std = abs(y - p).std()\n",
    "    plt.figure(num=None, figsize=(50, 5))\n",
    "    plt.title('MAE: %.2f, σ: %.2f' % (mae, std), fontsize=30)\n",
    "    plt.tick_params(labelsize=30)\n",
    "    plt.plot(y, label='Target')\n",
    "    plt.plot(p, label='Predict')\n",
    "    plt.legend(loc=3, prop={'size': 20})\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    # plt.title(mae_2)\n",
    "    plt.savefig(\"plot/\" + cctv_Id + \"/total.png\")\n",
    "    plt.close('all')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    print(\"Total MAE: %.3f\\n\" % mae, file=open(total_name, 'a'))\n",
    "    print(\"**********************\\n\", file=open(total_name, 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "316.4px",
    "left": "1130px",
    "right": "20px",
    "top": "47px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
