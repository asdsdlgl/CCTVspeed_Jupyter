{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Package<br>\n",
    "備註: 此程式會將frame數分為兩個model去訓練，並分別向左右延伸，找到最佳的訓練與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import Sequence, plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from models import *    #在jupyter中 改為 下面cell的 %run models.ipnyb\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "rn.seed(666)\n",
    "tf.set_random_seed(666)\n",
    "config = tf.ConfigProto()           # 此三行的目的是讓TensorFlow在運行過程中動態申請顯存，需要多少就申請多少\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model<br>\n",
    "包括 c3d、c3g25、c4g12、c5g6、c6g3、lrcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Class Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence): \n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs_1, list_IDs_3, labels, fnum, peak, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs_1 = list_IDs_1\n",
    "        self.list_IDs_3 = list_IDs_3\n",
    "        self.fnum = fnum\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.peak = peak\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs_1) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp_1, y = [self.list_IDs_1[k] for k in indexes],\\\n",
    "                             [self.labels[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X1 = self.__data_generation(list_IDs_temp_1, indexes)\n",
    "        return X1, np.array(y, dtype=int)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs_1))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(666)\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp_1, y_index):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X1 = []\n",
    "        # Generate data\n",
    "        for ID, y_ID in zip(list_IDs_temp_1, y_index):\n",
    "            # Store sample\n",
    "            x_1 = list(np.load(ID + '.npy'))\n",
    "            if len(x_1) >= 120:\n",
    "                x_1 = x_1[70:120]\n",
    "            else:\n",
    "                x_1 = x_1[:frame_num]\n",
    "            while len(x_1) < self.fnum:\n",
    "                x_1.append(np.zeros_like(x_1[0]))\n",
    "            X1.append(x_1)\n",
    "        X1 = np.array(X1)\n",
    "        return X1\n",
    "    \n",
    "class FrameInterval:\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.test_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.val_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "\n",
    "    def add_train(self, data, datatime):\n",
    "        self.train_df.loc[datatime] = [data]\n",
    "        self.train_df.index = pd.to_datetime(self.train_df.index)\n",
    "\n",
    "    def add_test(self, data, datatime):\n",
    "        self.test_df.loc[datatime] = [data]\n",
    "        self.test_df.index = pd.to_datetime(self.test_df.index)\n",
    "\n",
    "    def add_val(self, data, datatime):\n",
    "        self.val_df.loc[datatime] = [data]\n",
    "        self.val_df.index = pd.to_datetime(self.val_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define compare_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compare_mae(new, train, test, frame_num, mae, model, stage, dir):\n",
    "    global state\n",
    "    print(\"Stage %d\" % stage, file=open(total_name, 'a'))\n",
    "    print(\"Current MAE:\", \"%.3f\" % mae, file=open(total_name, 'a'))\n",
    "    new_train = sorted(new + train)\n",
    "    new_test = sorted(new + test)\n",
    "    X_train = []\n",
    "    y_train_index = []\n",
    "    X_test = []\n",
    "    y_test_index = []\n",
    "    X_val = []\n",
    "    y_val_index = []\n",
    "    for t in new_test:\n",
    "        X_test += FrameList[t].test_df['data'].tolist()\n",
    "        y_test_index += FrameList[t].test_df.index\n",
    "        X_val += FrameList[t].val_df['data'].tolist()\n",
    "        y_val_index += FrameList[t].val_df.index\n",
    "    for t in new_train:\n",
    "        X_train += FrameList[t].train_df['data'].tolist()\n",
    "        y_train_index += FrameList[t].train_df.index\n",
    "    X_train_fnum = frame_df.loc[y_train_index]\n",
    "    X_test_fnum = frame_df.loc[y_test_index]\n",
    "    X_val_fnum = frame_df.loc[y_val_index]\n",
    "    y_train = vd_df.loc[y_train_index, feature].values\n",
    "    y_test = vd_df.loc[y_test_index, feature].values\n",
    "    y_val = vd_df.loc[y_val_index, feature].values\n",
    "    print(\"New Train Length:\", len(X_train), file=open(total_name, 'a'))\n",
    "    print(\"New Test Length:\", len(X_test), file=open(total_name, 'a'))\n",
    "    print(\"New Val Length:\", len(X_val), file=open(total_name, 'a'))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_train_fnum = np.array(X_train_fnum)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    X_test_fnum = np.array(X_test_fnum)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    X_val_fnum = np.array(X_val_fnum)\n",
    "    y_val = np.array(y_val)\n",
    "    testing_generator = DataGenerator(X_test, X_test_fnum, y_test, frame_num, dir, False)\n",
    "\n",
    "    mae_2, mae_3 = 100, 100\n",
    "    if stage:\n",
    "        testPrdct = model.predict_generator(generator=testing_generator, max_queue_size=500, workers=32, verbose=1)\n",
    "        testPrdct = np.squeeze(testPrdct)\n",
    "        mae_2 = abs(y_test[:len(testPrdct)] - testPrdct).mean()\n",
    "        print(\"Merge Test MAE:\", \"%.3f\" % mae_2, file=open(total_name, 'a'))\n",
    "\n",
    "        plt.figure(num=None, figsize=(50, 5))\n",
    "        plt.tick_params(labelsize=30)\n",
    "        plt.plot(y_test, lw=2, label='Target')\n",
    "        plt.plot(testPrdct, lw=2, label='Predict')\n",
    "        plt.legend(loc=1, prop={'size': 20})\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        # plt.title(mae_2)\n",
    "        plt.savefig(\n",
    "            \"plot/\" + cctv_Id + \"/\" + dir + \"/stage_%d_merge1.png\" % stage)\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "    if state == 0:\n",
    "        training_generator = DataGenerator(X_train, X_train_fnum, y_train, frame_num, dir, True)\n",
    "        validation_generator = DataGenerator(X_val, X_val_fnum, y_val, frame_num, dir, True)\n",
    "        model_temp = model\n",
    "        history = model_temp.fit_generator(generator=training_generator, validation_data=validation_generator,\n",
    "                                           epochs=epochs, steps_per_epoch=len(X_train) // batch_size,\n",
    "                                           validation_steps=len(X_val) // batch_size,\n",
    "                                           max_queue_size=500, verbose=1, callbacks=[early_stopping], workers=32)\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=1)\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        plt.savefig(\n",
    "            \"plot/\" + cctv_Id + \"/\" + dir + \"/loss.png\")\n",
    "        testPrdct_2 = model_temp.predict_generator(generator=testing_generator, max_queue_size=500, workers=32, verbose=1)\n",
    "        testPrdct_2 = np.squeeze(testPrdct_2)\n",
    "        mae_3 = abs(y_test[:len(testPrdct_2)] - testPrdct_2).mean()\n",
    "        print(\"Merge All MAE:\", \"%.3f\" % mae_3, file=open(total_name, 'a'))\n",
    "        plt.figure(num=None, figsize=(50, 5))\n",
    "        plt.tick_params(labelsize=30)\n",
    "        plt.plot(y_test, lw=2, label='Target')\n",
    "        plt.plot(testPrdct_2, lw=2, label='Predict')\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        # plt.title(\"MAE: %.3f\" % mae_3)\n",
    "        plt.legend(loc=1, prop={'size': 20})\n",
    "        plt.savefig(\n",
    "            \"plot/\" + cctv_Id + \"/\" + dir + \"/stage_%d_merge2.png\" % stage)\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "    MAE = np.array([mae, mae_2, mae_3])\n",
    "    mini = MAE.argmin()\n",
    "    if stage == 0:\n",
    "        state = 0\n",
    "        print(\"State: Initial\", file=open(total_name, 'a'))\n",
    "        return new_train, new_test, mae_3, model_temp, False, testPrdct_2\n",
    "    if mini == 0:\n",
    "        state = 0\n",
    "        print(\"State: Stop merging\", file=open(total_name, 'a'))\n",
    "        return train, test, mae, model, True, None\n",
    "    elif mini == 1:\n",
    "        state = 1\n",
    "        print(\"State: Add test -> %d\" % new[0], file=open(total_name, 'a'))\n",
    "        return train, new_test, mae_2, model, False, testPrdct\n",
    "    else:\n",
    "        state = 0\n",
    "        print(\"State: Add train & test -> %d\" % new[0], file=open(total_name, 'a'))\n",
    "        return new_train, new_test, mae_3, model_temp, False, testPrdct_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basic setting<br>\n",
    "day1, train_dayLast: 訓練時間<br>\n",
    "test_day1, dayLast: 測試時間<br>\n",
    "val_day1, val_dayLast: 驗證時間<br>\n",
    "feature: 選擇真值(Spd_Max, Spd_Min, Spd_Avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = ['nfbVD-N5-N-16.900-M-PS', 'nfbVD-N5-N-18.313-M-PS', 'nfbVD-N5-N-22.510-M-PS', 'nfbVD-N5-N-25.310-M-PS',\n",
    "       'nfbVD-N5-S-18.312-M-PS', 'nfbVD-N5-S-19.677-M-PS', 'nfbVD-N5-S-21.063-M-PS', 'nfbVD-N5-S-23.910-M-PS']\n",
    "\n",
    "cctvs = ['nfbCCTV-N5-N-16.915-M', 'nfbCCTV-N5-N-18.308-M', 'nfbCCTV-N5-N-22.514-M', 'nfbCCTV-N5-N-25.309-M',\n",
    "         'nfbCCTV-N5-S-18.339-M', 'nfbCCTV-N5-S-19.7-M', 'nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "vds = [\"nfbVD-N5-S-21.063-M-PS\", \"nfbVD-N5-S-23.910-M-PS\", \"nfbVD-N5-N-25.310-M-PS\"]\n",
    "cctvs = ['nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M', 'nfbCCTV-N5-N-25.309-M']\n",
    "\n",
    "features = ['Spd_Max', 'Spd_Avg', 'Spd_Min']\n",
    "\n",
    "day1 = datetime.strptime('20190322 0600', \"%Y%m%d %H%M\")\n",
    "day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "train_dayLast = datetime.strptime('20190414 2359', \"%Y%m%d %H%M\")\n",
    "\n",
    "test_day1 = datetime.strptime('20190517 0600', \"%Y%m%d %H%M\")\n",
    "test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "dayLast = datetime.strptime('20190601 2359', \"%Y%m%d %H%M\")\n",
    "dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "\n",
    "val_day1 = datetime.strptime('20190602 0600', \"%Y%m%d %H%M\")\n",
    "val_dayLast = val_day1 + timedelta(days=6) - timedelta(minutes=1)\n",
    "\n",
    "# day1 = datetime.strptime('20190320 0600', \"%Y%m%d %H%M\")\n",
    "# day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "# train_dayLast = datetime.strptime('20190320 0630', \"%Y%m%d %H%M\")\n",
    "#\n",
    "# test_day1 = datetime.strptime('20190328 0600', \"%Y%m%d %H%M\")\n",
    "# test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "# dayLast = datetime.strptime('20190328 0630', \"%Y%m%d %H%M\")\n",
    "# dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "# Add VDs\n",
    "# dt_dtypes = {'Spd0': int, 'Spd1': int, 'Spd2': int, 'SpdMax': int, 'SpdMin': int, 'SpdAvg': int}\n",
    "# train_dt_index = pd.date_range(day1, periods=(train_dayLast - day1).total_seconds() // 60 + 1, freq='T')\n",
    "# test_dt_index = pd.date_range(test_dayFirst, periods=(dayLast - test_dayFirst).total_seconds() // 60 + 1, freq='T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo<br>\n",
    "model: 要訓練的神經網路模型<br>\n",
    "filename: 選擇vd真值的csv檔<br>\n",
    "備註: 此程式會將frame數分為兩個model去訓練，並分別向左右延伸，找到最佳的訓練與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Todo: Tune the fitting parameters\n",
    "feature = 'Spd_Max'\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='auto',\n",
    "                               verbose=0, restore_best_weights=True)\n",
    "\"\"\"Choose model\"\"\"\n",
    "name, model = c6g3()\n",
    "\n",
    "total_name = 'plot/All_log.txt'\n",
    "open(total_name, 'w').close()\n",
    "\n",
    "for cctv_Id, vd_Id in zip(cctvs, vds):\n",
    "    print(cctv_Id)\n",
    "    print(name)\n",
    "    print(cctv_Id, file=open(total_name, 'a'))\n",
    "    vdDir = 'D:/vd/' + vd_Id + '/vd_speed/'\n",
    "    cctvDir = 'E:/wcs/cctv/' + cctv_Id + \"/\"\n",
    "    filename = vdDir + 'vdspd_20190322_20190608.csv'\n",
    "    total_frames = cctvDir + \"frame_num.txt\"\n",
    "    vd_df = pd.read_csv(filename, infer_datetime_format=True, index_col=0)\n",
    "    vd_df.index = pd.to_datetime(vd_df.index)\n",
    "    vd_df = vd_df.shift(-1)\n",
    "    vd_df.loc[vd_df.index[len(vd_df)-1]] = 90\n",
    "    frame_df = pd.read_csv(total_frames, infer_datetime_format=True, index_col=0, header=None)\n",
    "    frame_df.index = pd.to_datetime(frame_df.index)\n",
    "    print(frame_df.index)\n",
    "    total_test_index = []\n",
    "    FrameList = []\n",
    "    with open(cctvDir + \"frame_interval.txt\") as f:\n",
    "        content = f.readlines()\n",
    "    frame_interval = np.array([x.strip() for x in content], dtype=int)\n",
    "    for i in range(len(content)):\n",
    "        FrameList.append(FrameInterval())\n",
    "    print(datetime.now())\n",
    "\n",
    "    if not os.path.exists(\"plot/\" + cctv_Id + \"/peak_1\"):\n",
    "        os.makedirs(\"plot/\" + cctv_Id + \"/peak_1\")\n",
    "        os.makedirs(\"plot/\" + cctv_Id + \"/peak_2\")\n",
    "    else:\n",
    "        for f in os.listdir(\"plot/\" + cctv_Id + \"/peak_1\"):\n",
    "            os.remove(\"plot/\" + cctv_Id + \"/peak_1/\" + f)\n",
    "        for f in os.listdir(\"plot/\" + cctv_Id + \"/peak_2\"):\n",
    "            os.remove(\"plot/\" + cctv_Id + \"/peak_2/\" + f)\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    val_count = 0\n",
    "    day = day1\n",
    "    while day <= train_dayLast:\n",
    "        # print(day)\n",
    "        if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\")\\\n",
    "                or day not in vd_df.index:\n",
    "            day += timedelta(minutes=1)\n",
    "            continue\n",
    "        train_count += 1\n",
    "        FrameList[int(frame_df.loc[day]) // 10]\\\n",
    "            .add_train(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\"), day)\n",
    "        day += timedelta(minutes=1)\n",
    "\n",
    "    day = test_day1\n",
    "    while day <= dayLast:\n",
    "        # print(day)\n",
    "        if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                or day not in vd_df.index:\n",
    "            day += timedelta(minutes=1)\n",
    "            continue\n",
    "        test_count += 1\n",
    "        FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "            .add_test(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\"), day)\n",
    "        total_test_index.append(day)\n",
    "        day += timedelta(minutes=1)\n",
    "\n",
    "    day = val_day1\n",
    "    while day <= val_dayLast:\n",
    "        # print(day)\n",
    "        if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                or day not in vd_df.index:\n",
    "            day += timedelta(minutes=1)\n",
    "            continue\n",
    "        val_count += 1\n",
    "        FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "            .add_val(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\"), day)\n",
    "        day += timedelta(minutes=1)\n",
    "\n",
    "    print(\"Total Train:\", train_count, file=open(total_name, 'a'))\n",
    "    print(\"Total test:\", test_count, file=open(total_name, 'a'))\n",
    "    print(\"Total val:\", val_count, file=open(total_name, 'a'))\n",
    "    print(\"Frame Num:\", len(frame_df))\n",
    "    shape = np.load(FrameList[5].train_df.iloc[0].values[0]+'.npy').shape[1:]\n",
    "    shape2 = np.load(FrameList[5].train_df.iloc[0].values[0]+'_timestamp.npy').shape[1:]\n",
    "    # my_models = [build_model(shape), build_model_2(shape), build_model_3(shape)]\n",
    "\n",
    "    ModelTrain_1 = [list(frame_interval).index(sorted(frame_interval)[-1])]\n",
    "    ModelTrain_1 = [ModelTrain_1[0] - 1] + ModelTrain_1\n",
    "    ModelTest_1 = ModelTrain_1.copy()\n",
    "\n",
    "    if abs(list(frame_interval).index(sorted(frame_interval)[-2]) -\n",
    "           list(frame_interval).index(sorted(frame_interval)[-1])) < 5:\n",
    "        ModelTrain_2 = [list(frame_interval).index(sorted(frame_interval)[-3])]\n",
    "    else:\n",
    "        ModelTrain_2 = [list(frame_interval).index(sorted(frame_interval)[-2])]\n",
    "    ModelTrain_2 = [ModelTrain_2[0] - 1] + ModelTrain_2\n",
    "    ModelTest_2 = ModelTrain_2.copy()\n",
    "\n",
    "    train = [ModelTrain_1, ModelTrain_2]\n",
    "    test = [ModelTest_1, ModelTest_2]\n",
    "\n",
    "    total_test = []\n",
    "\n",
    "    for peaks in range(2):\n",
    "        state = 0\n",
    "        Index = train[peaks][0] - 1\n",
    "        frame_num = train[peaks][-1]*10 if train[peaks][-1]*10 < 90 else 90\n",
    "        frame_num = 50\n",
    "        my_model = model\n",
    "        print(\"\\n*Peak %d\" % (peaks+1), file=open(total_name, 'a'))\n",
    "        train[peaks], test[peaks], currentMAE, my_model, isStopMerging, predict =\\\n",
    "            compare_mae([], train[peaks], test[peaks], frame_num, 100, my_model, 0, \"peak_%d\" % (peaks+1))\n",
    "        # file_name = \"plot/\" + cctv_Id + \"/\" + test_day1_str + '_' + dayLast_str + '/log.txt'\n",
    "        # open(file_name, 'w').close()\n",
    "        stages = 0\n",
    "        print(\"Frame Num:\", frame_num, file=open(total_name, 'a'))\n",
    "        print(\"Left Hand Side\", file=open(total_name, 'a'))\n",
    "        while Index >= 0 and not isStopMerging:\n",
    "            stages += 1\n",
    "            print(Index)\n",
    "            print(\"Index:\", Index, file=open(total_name, 'a'))\n",
    "            if len(FrameList[Index].train_df) > 60 and len(FrameList[Index].val_df) > 0 \\\n",
    "                    and len(FrameList[Index].test_df) > 0:\n",
    "                train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                    compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages, \"peak_%d\" % (peaks+1))\n",
    "                if p is not None:\n",
    "                    predict = p\n",
    "            else:\n",
    "                if len(FrameList[Index].test_df) > 0 and len(FrameList[Index].val_df) > 0:\n",
    "                    state = 1\n",
    "                    train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                        compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages,\n",
    "                                    \"peak_%d\" % (peaks + 1))\n",
    "                    if p is not None:\n",
    "                        predict = p\n",
    "                else:\n",
    "                    print(\"No data!\", file=open(total_name, 'a'))\n",
    "                    print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"\", file=open(total_name, 'a'))\n",
    "                    break\n",
    "            print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "            print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "            print(\"\", file=open(total_name, 'a'))\n",
    "            Index -= 1\n",
    "        state = 0\n",
    "        isStopMerging = False\n",
    "        print(\"Right Hand Side\", file=open(total_name, 'a'))\n",
    "        Index = train[peaks][-1] + 1\n",
    "        while Index < 21 and not isStopMerging:\n",
    "            stages += 1\n",
    "            print(Index)\n",
    "            print(\"Index:\", Index, file=open(total_name, 'a'))\n",
    "            if len(FrameList[Index].train_df) > 60 and len(FrameList[Index].val_df) > 0 \\\n",
    "                    and len(FrameList[Index].test_df) > 0:\n",
    "                train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                    compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages, \"peak_%d\" % (peaks+1))\n",
    "                if p is not None:\n",
    "                    predict = p\n",
    "            else:\n",
    "                if len(FrameList[Index].test_df) > 0 and len(FrameList[Index].val_df) > 0:\n",
    "                    state = 1\n",
    "                    train[peaks], test[peaks], currentMAE, my_model, isStopMerging, p = \\\n",
    "                        compare_mae([Index], train[peaks], test[peaks], frame_num, currentMAE, my_model, stages,\n",
    "                                    \"peak_%d\" % (peaks + 1))\n",
    "                    if p is not None:\n",
    "                        predict = p\n",
    "                else:\n",
    "                    print(\"No data!\", file=open(total_name, 'a'))\n",
    "                    print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "                    print(\"\", file=open(total_name, 'a'))\n",
    "                    break\n",
    "            print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "            print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "            print(\"\", file=open(total_name, 'a'))\n",
    "            Index += 1\n",
    "        total_test += test[peaks]\n",
    "        index = 0\n",
    "        for t in test[peaks]:\n",
    "            if len(FrameList[t].test_df) == 0:\n",
    "                continue\n",
    "            temp = predict[index:index+len(FrameList[t].test_df)].tolist()\n",
    "            while len(temp) < len(FrameList[t].test_df):\n",
    "                temp.append(temp[-1])\n",
    "            index += len(FrameList[t].test_df)\n",
    "            FrameList[t].test_df.insert(loc=0, column='predict', value=temp)\n",
    "        print(\"------------------\", file=open(total_name, 'a'))\n",
    "        my_model.save(\"plot/\" + cctv_Id + \"/peak_%d\" % (peaks+1) + \"/model.h5\")\n",
    "    predict_df = pd.DataFrame(columns=['value'])\n",
    "    for i in total_test_index:\n",
    "        for j in total_test:\n",
    "            if i in FrameList[j].test_df.index:\n",
    "                predict_df.loc[i] = [FrameList[j].test_df.loc[i]['predict']]\n",
    "    df = pd.DataFrame(index=total_test_index)\n",
    "    predict_all = np.array(predict_df.loc[total_test_index, 'value'].values)\n",
    "    vd_all = np.array(vd_df.loc[total_test_index, feature].values)\n",
    "    df.insert(loc=0, column='target', value=vd_all)\n",
    "    df.insert(loc=0, column='predict', value=predict_df)\n",
    "    df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='ffill')\n",
    "    df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='bfill')\n",
    "    df.to_csv(\"plot/\" + cctv_Id + \"/total_result.csv\")\n",
    "    print(\"Total predict:\", len(predict_df), file=open(total_name, 'a'))\n",
    "    print(\"Total test:\", len(vd_all), file=open(total_name, 'a'))\n",
    "    y = np.array(df['target'].values)\n",
    "    p = np.array(df['predict'].values)\n",
    "    mae = abs(y - p).mean()\n",
    "    std = abs(y - p).std()\n",
    "    plt.figure(num=None, figsize=(50, 5))\n",
    "    plt.title('MAE: %.2f, σ: %.2f' % (mae, std), fontsize=30)\n",
    "    plt.tick_params(labelsize=30)\n",
    "    plt.plot(y, label='Target')\n",
    "    plt.plot(p, label='Predict')\n",
    "    plt.legend(loc=3, prop={'size': 20})\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    # plt.title(mae_2)\n",
    "    plt.savefig(\"plot/\" + cctv_Id + \"/total.png\")\n",
    "    plt.close('all')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    print(\"Total MAE: %.3f\\n\" % mae, file=open(total_name, 'a'))\n",
    "    print(\"**********************\\n\", file=open(total_name, 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
