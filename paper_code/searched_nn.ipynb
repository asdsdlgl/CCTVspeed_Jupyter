{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Package<br>\n",
    "許多與pretrain_nn類似<br>\n",
    "models: 要訓練哪些模型(會依序訓練)<br>\n",
    "備註: 此程式會依據frame_index.txt來決定要使用哪些frame數的CCTV來訓練與測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "#from models_2 import *\n",
    "np.random.seed(666)\n",
    "rn.seed(666)\n",
    "tf.set_random_seed(666)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model<br>\n",
    "包括 c3d、c3g25、c4g12、c5g6、c6g3、lrcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Class Define<br>\n",
    "論文第10頁<br>\n",
    "參考 : 英文 https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly <br>\n",
    "      中文 https://blog.csdn.net/m0_37477175/article/details/79716312 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):#x     y\n",
    "    def __init__(self, list_IDs_1, labels, fnum, peak, shuffle=True): #list_IDs_1 ==> X , labels ==> Y\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs_1 = list_IDs_1\n",
    "        self.fnum = fnum\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.peak = peak\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(len(self.list_IDs_1) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]  #代表一個batch一個batch來分配\n",
    "        list_IDs_temp_1, y = [self.list_IDs_1[k] for k in indexes],\\\n",
    "                             [self.labels[k] for k in indexes]                      #indexes對應的 x , y\n",
    "        X1 = self.__data_generation(list_IDs_temp_1, indexes)\n",
    "        return X1, np.array(y, dtype=int)                      #return x,y to generator\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs_1))         #決定indexes是否要shuffle \n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(666)\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp_1, y_index):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
    "        X1 = []\n",
    "        for ID, y_ID in zip(list_IDs_temp_1, y_index):\n",
    "            # Store sample\n",
    "            x_1 = list(np.load(ID.split(\" \")[0] + '.npy')) #different with pretrain_nn , split\n",
    "            if len(x_1) >= 120:\n",
    "                x_1 = x_1[70:120]\n",
    "            else:\n",
    "                x_1 = x_1[:frame_num]\n",
    "            while len(x_1) < self.fnum:\n",
    "                x_1.append(np.zeros_like(x_1[0]))\n",
    "            x_1 = add_noises(x_1, ID.split(\" \")[1])         #different with pretrain_nn , add noise\n",
    "            X1.append(x_1)  \n",
    "        X1 = np.array(X1)\n",
    "        return X1\n",
    "\n",
    "class FrameInterval:\n",
    "    \"\"\"Classify data by frame number\"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.test_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.val_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "\n",
    "    def add_train(self, data, datatime):\n",
    "        df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        df.loc[datatime] = [data]\n",
    "        self.train_df = self.train_df.append(df)\n",
    "        self.train_df.index = pd.to_datetime(self.train_df.index)\n",
    "\n",
    "    def add_test(self, data, datatime):\n",
    "        self.test_df.loc[datatime] = [data]\n",
    "        self.test_df.index = pd.to_datetime(self.test_df.index)\n",
    "\n",
    "    def add_val(self, data, datatime):\n",
    "        self.val_df.loc[datatime] = [data]\n",
    "        self.val_df.index = pd.to_datetime(self.val_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define add_noises function<br> \n",
    "輸入input格式為 D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644 + \" 0\" or \" 1\" <br>\n",
    "[\"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644\",\"0\"] or [\"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644\",\"1\"] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noises(images, ind):             #將tag為1的加入noise 的training set 為0的維持一樣\n",
    "    if ind == '0':  \n",
    "        \"\"\"Original\"\"\"\n",
    "        return images\n",
    "    elif ind == '1':\n",
    "        \"\"\"Adding noise\"\"\"\n",
    "        resize = []\n",
    "        rand1 = np.random.randint(0, 4)  #隨機選擇其中一邊\n",
    "        rand2 = np.random.randint(0, 16) #裁減0~15個pixel\n",
    "        for img in images:\n",
    "            if rand1 == 0:\n",
    "                img = img[rand2:, :]\n",
    "                img = np.concatenate((img, np.zeros((rand2, 106, 1))), axis=0) #向上或向下偏移\n",
    "            elif rand1 == 1:\n",
    "                img = img[:, rand2:]\n",
    "                img = np.concatenate((img, np.zeros((72, rand2, 1))), axis=1)  #向左或向右偏移\n",
    "            elif rand1 == 2:\n",
    "                img = img[:72-rand2, :]\n",
    "                img = np.concatenate((np.zeros((rand2, 106, 1)), img), axis=0)\n",
    "            else:\n",
    "                img = img[:, :106-rand2]\n",
    "                img = np.concatenate((np.zeros((72, rand2, 1)), img), axis=1)\n",
    "            resize.append(img)\n",
    "        return resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define compare_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mae(train, test, frame_num, dir, model_temp):\n",
    "    X_train = []\n",
    "    y_train_index = []\n",
    "    X_test = []\n",
    "    y_test_index = []\n",
    "    X_val = []\n",
    "    y_val_index = []\n",
    "    for t in test:\n",
    "        X_test += FrameList[t].test_df['data'].tolist()\n",
    "        y_test_index += FrameList[t].test_df.index\n",
    "        X_val += FrameList[t].val_df['data'].tolist()\n",
    "        y_val_index += FrameList[t].val_df.index\n",
    "    for t in train:\n",
    "        X_train += FrameList[t].train_df['data'].tolist()\n",
    "        y_train_index += FrameList[t].train_df.index\n",
    "    y_train = vd_df.loc[y_train_index, feature].values\n",
    "    y_test = vd_df.loc[y_test_index, feature].values\n",
    "    y_val = vd_df.loc[y_val_index, feature].values\n",
    "    print(\"Train Length:\", len(X_train), file=open(total_name, 'a'))\n",
    "    print(\"Test Length:\", len(X_test), file=open(total_name, 'a'))\n",
    "    print(\"Val Length:\", len(X_val), file=open(total_name, 'a'))\n",
    "\n",
    "    X_train = np.array(X_train)  # npy的data\n",
    "    y_train = np.array(y_train)  #真值\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    training_generator = DataGenerator(X_train, y_train, frame_num, dir, True)\n",
    "    testing_generator = DataGenerator(X_test, y_test, frame_num, dir, False)\n",
    "    validation_generator = DataGenerator(X_val, y_val, frame_num, dir, True)\n",
    "\n",
    "    history = model_temp.fit_generator(generator=training_generator, validation_data=validation_generator,\n",
    "                                       epochs=epochs, steps_per_epoch=len(X_train) // batch_size,\n",
    "                                       validation_steps=len(X_val) // batch_size,\n",
    "                                       max_queue_size=200, verbose=1, callbacks=[early_stopping], workers=4)\n",
    "    print(\"Epochs: %d\" % len(history.history['loss']), file=open(total_name, 'a'))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc=1)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(\n",
    "        \"plot/\" + name + \"/\" + cctv_Id + \"/\" + dir + \"/loss.png\")\n",
    "    testPrdct = model_temp.predict_generator(generator=testing_generator, max_queue_size=200, workers=4, verbose=1,\n",
    "                                             steps=len(X_test) // batch_size)\n",
    "    testPrdct = np.squeeze(testPrdct)\n",
    "    mae = abs(y_test[:len(testPrdct)] - testPrdct).mean()\n",
    "    print(\"MAE:\", \"%.2f\" % mae, file=open(total_name, 'a'))\n",
    "    plt.figure(num=None, figsize=(50, 5))\n",
    "    std = abs(y_test[:len(testPrdct)] - testPrdct).std()\n",
    "    plt.title('MAE: %.2f, σ: %.2f' % (mae, std), fontsize=30)\n",
    "    plt.tick_params(labelsize=30)\n",
    "    plt.plot(y_test, lw=2, label='Target')\n",
    "    plt.plot(testPrdct, lw=2, label='Predict')\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    # plt.title(\"MAE: %.3f\" % mae_3)\n",
    "    plt.legend(loc=1, prop={'size': 20})\n",
    "    plt.savefig(\n",
    "        \"plot/\" + name + \"/\" + cctv_Id + \"/\" + dir + \"/total.png\")\n",
    "    plt.close('all')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    return model_temp, testPrdct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basic setting<br>\n",
    "day1, train_dayLast: 訓練時間<br>\n",
    "test_day1, dayLast: 測試時間<br>\n",
    "val_day1, val_dayLast: 驗證時間<br>\n",
    "feature: 選擇真值(Spd_Max, Spd_Min, Spd_Avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VDs and CCTVs\"\"\"\n",
    "#vds = ['nfbVD-N5-N-16.900-M-PS', 'nfbVD-N5-N-18.313-M-PS', 'nfbVD-N5-N-22.510-M-PS', 'nfbVD-N5-N-25.310-M-PS',\n",
    "#       'nfbVD-N5-S-18.312-M-PS', 'nfbVD-N5-S-19.677-M-PS', 'nfbVD-N5-S-21.063-M-PS', 'nfbVD-N5-S-23.910-M-PS']\n",
    "# vds = ['nfbVD-N5-S-18.312-M-PS', 'nfbVD-N5-S-19.677-M-PS', 'nfbVD-N5-S-21.063-M-PS', 'nfbVD-N5-S-23.910-M-PS']\n",
    "vds = [\"nfbVD-N5-S-21.063-M-PS\", \"nfbVD-N5-S-23.910-M-PS\"]\n",
    "\n",
    "#cctvs = ['nfbCCTV-N5-N-16.915-M', 'nfbCCTV-N5-N-18.308-M', 'nfbCCTV-N5-N-22.514-M', 'nfbCCTV-N5-N-25.309-M',\n",
    "#         'nfbCCTV-N5-S-18.339-M', 'nfbCCTV-N5-S-19.7-M', 'nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "# cctvs = ['nfbCCTV-N5-S-18.339-M', 'nfbCCTV-N5-S-19.7-M', 'nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "cctvs = ['nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "\n",
    "# Todo: choose data set, tune training params\n",
    "features = ['Spd_Max', 'Spd_Avg', 'Spd_Min']\n",
    "\n",
    "\n",
    "day1 = datetime.strptime('20190728 0600', \"%Y%m%d %H%M\")\n",
    "day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "train_dayLast = datetime.strptime('20190729 2359', \"%Y%m%d %H%M\")\n",
    "\n",
    "test_day1 = datetime.strptime('20190728 0600', \"%Y%m%d %H%M\")\n",
    "test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "dayLast = datetime.strptime('20190729 2359', \"%Y%m%d %H%M\")\n",
    "dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "\n",
    "val_day1 = datetime.strptime('20190729 0600', \"%Y%m%d %H%M\")\n",
    "val_dayLast = val_day1 + timedelta(days=6) - timedelta(minutes=1)\n",
    "\n",
    "\n",
    "# day1 = datetime.strptime('20190322 0600', \"%Y%m%d %H%M\")\n",
    "# day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "# train_dayLast = datetime.strptime('20190414 2359', \"%Y%m%d %H%M\")\n",
    "\n",
    "# test_day1 = datetime.strptime('20190517 0600', \"%Y%m%d %H%M\")\n",
    "# test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "# dayLast = datetime.strptime('20190601 2359', \"%Y%m%d %H%M\")\n",
    "# dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "\n",
    "# val_day1 = datetime.strptime('20190602 0600', \"%Y%m%d %H%M\")\n",
    "# val_dayLast = val_day1 + timedelta(days=6) - timedelta(minutes=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo<br>\n",
    "model: 要訓練的神經網路模型<br>\n",
    "filename: 選擇vd真值的csv檔<br>\n",
    "備註: 此程式會將frame數分為兩個model去訓練，並分別向左右延伸，找到最佳的訓練與測試集<br>\n",
    "frame_num : 取50frame 論文第10頁<br>\n",
    "frame_index.txt欄位說明: [CCTV,peak1 train, peak1 test, peak2 train, peak2 test] *此檔案須根據pretrain_nn的結果來手動編輯<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50, 72, 106, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 50, 36, 53, 32)    896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 36, 53, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 25, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 25, 18, 26, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25, 18, 26, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 12, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 12, 9, 13, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 12, 9, 13, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 6, 4, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6, 4, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 6, 4, 6, 32)       27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 4, 6, 32)       128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6, 4, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 768)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 6, 128)            344448    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 6, 128)            98688     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 654,209\n",
      "Trainable params: 653,889\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 72, 106, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 50, 36, 53, 32)    896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50, 36, 53, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 50, 18, 26, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 50, 18, 26, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 25, 9, 13, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 25, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 25, 9, 13, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 25, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 12, 4, 6, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 12, 4, 6, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 12, 768)           0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 12, 128)           344448    \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 12, 128)           98688     \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 626,401\n",
      "Trainable params: 626,145\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50, 72, 106, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 50, 36, 53, 32)    896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 50, 36, 53, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 50, 18, 26, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 50, 18, 26, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 25, 9, 13, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 25, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 25, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 25, 3744)          0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 25, 128)           1487232   \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 25, 128)           98688     \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,741,377\n",
      "Trainable params: 1,741,185\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50, 72, 106, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 50, 36, 53, 32)    320       \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 50, 18, 26, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 50, 18, 26, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 50, 18, 26, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 50, 9, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 50, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 50, 9, 13, 32)     9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 50, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 50, 9, 13, 32)     9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 50, 9, 13, 32)     128       \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 50, 4, 6, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 50, 4, 6, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 50, 4, 6, 32)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 50, 4, 6, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 50, 4, 6, 32)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 50, 2, 3, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 50, 192)           0         \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 50, 128)           123264    \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 50, 128)           98688     \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 368,097\n",
      "Trainable params: 367,713\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "nfbCCTV-N5-S-21.048-M\n",
      "2019-08-26 02:22:26.569792\n",
      "Frame Num: 1081\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "19/19 [==============================] - 433s 23s/step - loss: 70.7290 - acc: 0.0000e+00 - val_loss: 65.3983 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 159s 8s/step - loss: 63.4504 - acc: 0.0000e+00 - val_loss: 61.6262 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 158s 8s/step - loss: 60.2953 - acc: 0.0000e+00 - val_loss: 58.7652 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 158s 8s/step - loss: 57.5218 - acc: 0.0000e+00 - val_loss: 56.1041 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 158s 8s/step - loss: 54.8929 - acc: 0.0000e+00 - val_loss: 53.5100 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 157s 8s/step - loss: 52.3193 - acc: 0.0000e+00 - val_loss: 50.9591 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 159s 8s/step - loss: 49.7818 - acc: 0.0000e+00 - val_loss: 48.4352 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 158s 8s/step - loss: 47.2677 - acc: 0.0000e+00 - val_loss: 45.9326 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 159s 8s/step - loss: 44.7729 - acc: 0.0000e+00 - val_loss: 43.4448 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 161s 8s/step - loss: 42.2295 - acc: 0.0000e+00 - val_loss: 40.8735 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 161s 8s/step - loss: 39.7003 - acc: 0.0000e+00 - val_loss: 38.3603 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 173s 9s/step - loss: 37.1938 - acc: 0.0000e+00 - val_loss: 35.8615 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 159s 8s/step - loss: 34.6909 - acc: 0.0000e+00 - val_loss: 33.3075 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 160s 8s/step - loss: 32.1235 - acc: 0.0000e+00 - val_loss: 30.7714 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 165s 9s/step - loss: 29.5932 - acc: 0.0000e+00 - val_loss: 28.2482 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "18/19 [===========================>..] - ETA: 7s - loss: 27.1848 - acc: 0.0000e+00 "
     ]
    }
   ],
   "source": [
    "feature = 'Spd_Avg'\n",
    "frame_num = 50\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='auto',\n",
    "                               verbose=0, restore_best_weights=True)\n",
    "\n",
    "model_count = 0\n",
    "\n",
    "\"\"\"Models to train\"\"\"\n",
    "#models = [c3d(), c6g3(), c5g6(), c4g12(), c3g25(), lrcn()]\n",
    "models = [c5g6(), c4g12(), c3g25(), lrcn()]\n",
    "\n",
    "for name, model in models:\n",
    "    if not os.path.exists(\"plot/\" + name):\n",
    "        os.makedirs(\"plot/\" + name)\n",
    "    total_name = 'plot/' + name + '/log.txt'\n",
    "    open(total_name, 'w').close()\n",
    "    for Id, (cctv_Id, vd_Id) in enumerate(zip(cctvs, vds)):\n",
    "        print(cctv_Id)\n",
    "        print(cctv_Id, file=open(total_name, 'a'))\n",
    "        if not os.path.exists(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1\"):\n",
    "            os.makedirs(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1\")\n",
    "            os.makedirs(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_2\")\n",
    "        else:\n",
    "            for f in os.listdir(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1\"):\n",
    "                os.remove(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1/\" + f)\n",
    "            for f in os.listdir(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_2\"):\n",
    "                os.remove(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_2/\" + f)\n",
    "        vdDir = 'D:/vd/' + vd_Id + '/vd_speed/'\n",
    "        cctvDir = 'D:/wcs/cctv/' + cctv_Id + \"/\"\n",
    "        filename = vdDir + 'vdspd_20190728_20190729.csv'\n",
    "        total_frames = cctvDir + \"frame_num.txt\"\n",
    "        vd_df = pd.read_csv(filename, infer_datetime_format=True, index_col=0)\n",
    "        vd_df.index = pd.to_datetime(vd_df.index)\n",
    "        vd_df = vd_df.shift(-1)                              #往上位移一位 因為資料取得有延遲\n",
    "        vd_df.loc[vd_df.index[len(vd_df)-1]] = 90            #將位移完後的nan填補90\n",
    "        frame_df = pd.read_csv(total_frames, infer_datetime_format=True, index_col=0, header=None)\n",
    "        frame_df.index = pd.to_datetime(frame_df.index)      #必須要用to_datetime 因為loc才能找到 從2019/07/29 ---> 2019-07-29\n",
    "        total_test_index = []\n",
    "        FrameList = []\n",
    "        with open(cctvDir + \"frame_interval.txt\") as f:\n",
    "            content = f.readlines()\n",
    "        frame_interval = np.array([x.strip() for x in content], dtype=int)\n",
    "        for i in range(len(content)):\n",
    "            FrameList.append(FrameInterval())\n",
    "        print(datetime.now())\n",
    "        train_count = 0\n",
    "        test_count = 0\n",
    "        val_count = 0\n",
    "        day = day1                       #first day of training day\n",
    "        while day <= train_dayLast:\n",
    "            # print(day)\n",
    "            if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\")\\\n",
    "                    or day not in vd_df.index:\n",
    "                day += timedelta(minutes=1)\n",
    "                continue\n",
    "            train_count += 1\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_train(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 0\", day)\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_train(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 1\", day)\n",
    "             #     \"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644 + \" 0\" or \" 1\" <- 有空白 , \"2019-07-29 16:44:00\"\n",
    "            day += timedelta(minutes=1)                                      #\"以方便後面做切割判斷\"\n",
    "\n",
    "        day = test_day1                 #first day of testing day\n",
    "        while day <= dayLast:\n",
    "            # print(day)\n",
    "            if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                    or day not in vd_df.index:\n",
    "                day += timedelta(minutes=1)\n",
    "                continue\n",
    "            test_count += 1\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_test(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 0\", day)\n",
    "            total_test_index.append(day)\n",
    "            day += timedelta(minutes=1)\n",
    "\n",
    "        day = val_day1                  #first day of validating day\n",
    "        while day <= val_dayLast:\n",
    "            # print(day)\n",
    "            if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                    or day not in vd_df.index:\n",
    "                day += timedelta(minutes=1)\n",
    "                continue\n",
    "            val_count += 1\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_val(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 0\", day)\n",
    "            day += timedelta(minutes=1)\n",
    "\n",
    "        print(\"Total Train:\", train_count, file=open(total_name, 'a'))\n",
    "        print(\"Total test:\", test_count, file=open(total_name, 'a'))\n",
    "        print(\"Total val:\", val_count, file=open(total_name, 'a'))\n",
    "        print(\"Frame Num:\", len(frame_df))\n",
    "\n",
    "        ModelTrain_1 = []\n",
    "        ModelTest_1 = []\n",
    "        ModelTrain_2 = []\n",
    "        ModelTest_2 = []\n",
    "        with open('frame_index.txt', 'r') as f:        #frame_index.txt\"是手動編集的\n",
    "            for line in f:\n",
    "                if cctv_Id in line:\n",
    "                    t1, t2, t3, t4 = line.split(\",\")[1:]\n",
    "                    for i in t1.split(\"/\"):\n",
    "                        ModelTrain_1.append(int(i))\n",
    "                    for i in t2.split(\"/\"):\n",
    "                        ModelTest_1.append(int(i))\n",
    "                    for i in t3.split(\"/\"):\n",
    "                        ModelTrain_2.append(int(i))\n",
    "                    for i in t4.split(\"/\"):\n",
    "                        ModelTest_2.append(int(i))\n",
    "\n",
    "        train = [ModelTrain_1, ModelTrain_2]\n",
    "        test = [ModelTest_1, ModelTest_2]\n",
    "        # train = [ModelTrain_2]\n",
    "        # test = [ModelTest_2]\n",
    "\n",
    "        total_test = []\n",
    "\n",
    "        for peaks in range(2):\n",
    "            print(\"\\n*Peak %d\" % (peaks+1), file=open(total_name, 'a'))\n",
    "            print(\"Frame Num:\", frame_num, file=open(total_name, 'a'))\n",
    "            print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "            print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "            my_model, predict = compare_mae(train[peaks], test[peaks], frame_num, \"peak_%d\" % (peaks+1), model)\n",
    "            my_model.save(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_%d\" % (peaks+1) + \"/model.h5\")\n",
    "            total_test += test[peaks]\n",
    "            index = 0\n",
    "            for t in test[peaks]:\n",
    "                temp = predict[index:index+len(FrameList[t].test_df)].tolist()\n",
    "                while len(temp) < len(FrameList[t].test_df):\n",
    "                    temp.append(temp[-1])\n",
    "                index += len(FrameList[t].test_df)\n",
    "                FrameList[t].test_df.insert(loc=0, column='predict', value=temp)\n",
    "        predict_df = pd.DataFrame(columns=['value'])\n",
    "        for i in total_test_index:\n",
    "            for j in total_test:\n",
    "                if i in FrameList[j].test_df.index: \n",
    "                    predict_df.loc[i] = [FrameList[j].test_df.loc[i]['predict']]\n",
    "        df = pd.DataFrame(index=total_test_index)\n",
    "        predict_all = np.array(predict_df.loc[total_test_index, 'value'].values)\n",
    "        vd_all = np.array(vd_df.loc[total_test_index, feature].values)\n",
    "        df.insert(loc=0, column='target', value=vd_all)\n",
    "        df.insert(loc=0, column='predict', value=predict_df)\n",
    "        df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='ffill') #處有少數天數有nan的 因為frame數不再total_test中\n",
    "        # Fill first predicted value if needed\n",
    "        df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='bfill')\n",
    "        df.to_csv(\"plot/\" + name + \"/\" + cctv_Id + \"/total_result.csv\")\n",
    "        print(\"Total predict:\", len(predict_df), file=open(total_name, 'a'))\n",
    "        print(\"Total test:\", len(vd_all), file=open(total_name, 'a'))\n",
    "        y = np.array(df['target'].values)\n",
    "        p = np.array(df['predict'].values)\n",
    "        mae = abs(y - p).mean()\n",
    "        plt.figure(num=None, figsize=(50, 5))\n",
    "        std = abs(y - p).std()\n",
    "        plt.title('MAE: %.2f, σ: %.2f' % (mae, std), fontsize=30)\n",
    "        plt.tick_params(labelsize=30)\n",
    "        plt.plot(y, label='Target')\n",
    "        plt.plot(p, label='Predict')\n",
    "        plt.legend(loc=3, prop={'size': 20})\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        # plt.title(mae_2)\n",
    "        plt.savefig(\"plot/\" + name + \"/\" + cctv_Id + \"/total.png\")\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        print(\"Total Std: %.2f\" % std, file=open(total_name, 'a'))\n",
    "        print(\"Total MAE: %.2f\\n\" % mae, file=open(total_name, 'a'))\n",
    "        print(\"**********************\\n\", file=open(total_name, 'a'))\n",
    "    model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
