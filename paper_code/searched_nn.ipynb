{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Package<br>\n",
    "許多與pretrain_nn類似<br>\n",
    "models: 要訓練哪些模型(會依序訓練)<br>\n",
    "備註: 此程式會依據frame_index.txt來決定要使用哪些frame數的CCTV來訓練與測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from models_2 import *\n",
    "np.random.seed(666)\n",
    "rn.seed(666)\n",
    "tf.set_random_seed(666)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model<br>\n",
    "包括 c3d、c3g25、c4g12、c5g6、c6g3、lrcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Class Define<br>\n",
    "論文第10頁<br>\n",
    "參考 : 英文 https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly <br>\n",
    "      中文 https://blog.csdn.net/m0_37477175/article/details/79716312 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs_1, labels, fnum, peak, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs_1 = list_IDs_1\n",
    "        self.fnum = fnum\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.peak = peak\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(len(self.list_IDs_1) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        list_IDs_temp_1, y = [self.list_IDs_1[k] for k in indexes],\\\n",
    "                             [self.labels[k] for k in indexes]\n",
    "        X1 = self.__data_generation(list_IDs_temp_1, indexes)\n",
    "        return X1, np.array(y, dtype=int)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs_1))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(666)\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp_1, y_index):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
    "        X1 = []\n",
    "        for ID, y_ID in zip(list_IDs_temp_1, y_index):\n",
    "            # Store sample\n",
    "            x_1 = list(np.load(ID.split(\" \")[0] + '.npy')) #different with pretrain_nn , split\n",
    "            if len(x_1) >= 120:\n",
    "                x_1 = x_1[70:120]\n",
    "            else:\n",
    "                x_1 = x_1[:frame_num]\n",
    "            while len(x_1) < self.fnum:\n",
    "                x_1.append(np.zeros_like(x_1[0]))\n",
    "            x_1 = add_noises(x_1, ID.split(\" \")[1])         #different with pretrain_nn , add noise\n",
    "            X1.append(x_1)  \n",
    "        X1 = np.array(X1)\n",
    "        return X1\n",
    "\n",
    "class FrameInterval:\n",
    "    \"\"\"Classify data by frame number\"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.test_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        self.val_df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "\n",
    "    def add_train(self, data, datatime):\n",
    "        df = pd.DataFrame(columns=['data'], dtype=str)\n",
    "        df.loc[datatime] = [data]\n",
    "        self.train_df = self.train_df.append(df)\n",
    "        self.train_df.index = pd.to_datetime(self.train_df.index)\n",
    "\n",
    "    def add_test(self, data, datatime):\n",
    "        self.test_df.loc[datatime] = [data]\n",
    "        self.test_df.index = pd.to_datetime(self.test_df.index)\n",
    "\n",
    "    def add_val(self, data, datatime):\n",
    "        self.val_df.loc[datatime] = [data]\n",
    "        self.val_df.index = pd.to_datetime(self.val_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define add_noises function<br> \n",
    "輸入input格式為 D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644 + \" 0\" or \" 1\" <br>\n",
    "[\"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644\",\"0\"] or [\"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644\",\"1\"] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noises(images, ind):             #將tag為1的加入noise 的training set 為0的維持一樣\n",
    "    if ind == '0':  \n",
    "        \"\"\"Original\"\"\"\n",
    "        return images\n",
    "    elif ind == '1':\n",
    "        \"\"\"Adding noise\"\"\"\n",
    "        resize = []\n",
    "        rand1 = np.random.randint(0, 4)  #隨機選擇其中一邊\n",
    "        rand2 = np.random.randint(0, 16) #裁減0~15個pixel\n",
    "        for img in images:\n",
    "            if rand1 == 0:\n",
    "                img = img[rand2:, :]\n",
    "                img = np.concatenate((img, np.zeros((rand2, 106, 1))), axis=0) #向上或向下偏移\n",
    "            elif rand1 == 1:\n",
    "                img = img[:, rand2:]\n",
    "                img = np.concatenate((img, np.zeros((72, rand2, 1))), axis=1)  #向左或向右偏移\n",
    "            elif rand1 == 2:\n",
    "                img = img[:72-rand2, :]\n",
    "                img = np.concatenate((np.zeros((rand2, 106, 1)), img), axis=0)\n",
    "            else:\n",
    "                img = img[:, :106-rand2]\n",
    "                img = np.concatenate((np.zeros((72, rand2, 1)), img), axis=1)\n",
    "            resize.append(img)\n",
    "        return resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define compare_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mae(train, test, frame_num, dir, model_temp):\n",
    "    X_train = []\n",
    "    y_train_index = []\n",
    "    X_test = []\n",
    "    y_test_index = []\n",
    "    X_val = []\n",
    "    y_val_index = []\n",
    "    for t in test:\n",
    "        X_test += FrameList[t].test_df['data'].tolist()\n",
    "        y_test_index += FrameList[t].test_df.index\n",
    "        X_val += FrameList[t].val_df['data'].tolist()\n",
    "        y_val_index += FrameList[t].val_df.index\n",
    "    for t in train:\n",
    "        X_train += FrameList[t].train_df['data'].tolist()\n",
    "        y_train_index += FrameList[t].train_df.index\n",
    "    y_train = vd_df.loc[y_train_index, feature].values\n",
    "    y_test = vd_df.loc[y_test_index, feature].values\n",
    "    y_val = vd_df.loc[y_val_index, feature].values\n",
    "    print(\"Train Length:\", len(X_train), file=open(total_name, 'a'))\n",
    "    print(\"Test Length:\", len(X_test), file=open(total_name, 'a'))\n",
    "    print(\"Val Length:\", len(X_val), file=open(total_name, 'a'))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    training_generator = DataGenerator(X_train, y_train, frame_num, dir, True)\n",
    "    testing_generator = DataGenerator(X_test, y_test, frame_num, dir, False)\n",
    "    validation_generator = DataGenerator(X_val, y_val, frame_num, dir, True)\n",
    "\n",
    "    history = model_temp.fit_generator(generator=training_generator, validation_data=validation_generator,\n",
    "                                       epochs=epochs, steps_per_epoch=len(X_train) // batch_size,\n",
    "                                       validation_steps=len(X_val) // batch_size,\n",
    "                                       max_queue_size=200, verbose=1, callbacks=[early_stopping], workers=4)\n",
    "    print(\"Epochs: %d\" % len(history.history['loss']), file=open(total_name, 'a'))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(loc=1)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(\n",
    "        \"plot/\" + name + \"/\" + cctv_Id + \"/\" + dir + \"/loss.png\")\n",
    "    testPrdct = model_temp.predict_generator(generator=testing_generator, max_queue_size=200, workers=4, verbose=1,\n",
    "                                             steps=len(X_test) // batch_size)\n",
    "    testPrdct = np.squeeze(testPrdct)\n",
    "    mae = abs(y_test[:len(testPrdct)] - testPrdct).mean()\n",
    "    print(\"MAE:\", \"%.2f\" % mae, file=open(total_name, 'a'))\n",
    "    plt.figure(num=None, figsize=(50, 5))\n",
    "    std = abs(y_test[:len(testPrdct)] - testPrdct).std()\n",
    "    plt.title('MAE: %.2f, σ: %.2f' % (mae, std), fontsize=30)\n",
    "    plt.tick_params(labelsize=30)\n",
    "    plt.plot(y_test, lw=2, label='Target')\n",
    "    plt.plot(testPrdct, lw=2, label='Predict')\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    # plt.title(\"MAE: %.3f\" % mae_3)\n",
    "    plt.legend(loc=1, prop={'size': 20})\n",
    "    plt.savefig(\n",
    "        \"plot/\" + name + \"/\" + cctv_Id + \"/\" + dir + \"/total.png\")\n",
    "    plt.close('all')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    return model_temp, testPrdct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basic setting<br>\n",
    "day1, train_dayLast: 訓練時間<br>\n",
    "test_day1, dayLast: 測試時間<br>\n",
    "val_day1, val_dayLast: 驗證時間<br>\n",
    "feature: 選擇真值(Spd_Max, Spd_Min, Spd_Avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VDs and CCTVs\"\"\"\n",
    "#vds = ['nfbVD-N5-N-16.900-M-PS', 'nfbVD-N5-N-18.313-M-PS', 'nfbVD-N5-N-22.510-M-PS', 'nfbVD-N5-N-25.310-M-PS',\n",
    "#       'nfbVD-N5-S-18.312-M-PS', 'nfbVD-N5-S-19.677-M-PS', 'nfbVD-N5-S-21.063-M-PS', 'nfbVD-N5-S-23.910-M-PS']\n",
    "# vds = ['nfbVD-N5-S-18.312-M-PS', 'nfbVD-N5-S-19.677-M-PS', 'nfbVD-N5-S-21.063-M-PS', 'nfbVD-N5-S-23.910-M-PS']\n",
    "vds = [\"nfbVD-N5-S-21.063-M-PS\", \"nfbVD-N5-S-23.910-M-PS\"]\n",
    "\n",
    "#cctvs = ['nfbCCTV-N5-N-16.915-M', 'nfbCCTV-N5-N-18.308-M', 'nfbCCTV-N5-N-22.514-M', 'nfbCCTV-N5-N-25.309-M',\n",
    "#         'nfbCCTV-N5-S-18.339-M', 'nfbCCTV-N5-S-19.7-M', 'nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "# cctvs = ['nfbCCTV-N5-S-18.339-M', 'nfbCCTV-N5-S-19.7-M', 'nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "cctvs = ['nfbCCTV-N5-S-21.048-M', 'nfbCCTV-N5-S-23.896-M']\n",
    "\n",
    "# Todo: choose data set, tune training params\n",
    "features = ['Spd_Max', 'Spd_Avg', 'Spd_Min']\n",
    "\n",
    "\n",
    "day1 = datetime.strptime('20190728 0600', \"%Y%m%d %H%M\")\n",
    "day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "train_dayLast = datetime.strptime('20190729 2359', \"%Y%m%d %H%M\")\n",
    "\n",
    "test_day1 = datetime.strptime('20190728 0600', \"%Y%m%d %H%M\")\n",
    "test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "dayLast = datetime.strptime('20190729 2359', \"%Y%m%d %H%M\")\n",
    "dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "\n",
    "val_day1 = datetime.strptime('20190729 0600', \"%Y%m%d %H%M\")\n",
    "val_dayLast = val_day1 + timedelta(days=6) - timedelta(minutes=1)\n",
    "\n",
    "\n",
    "# day1 = datetime.strptime('20190322 0600', \"%Y%m%d %H%M\")\n",
    "# day1_str = day1.strftime(\"%Y%m%d %H%M\")\n",
    "# train_dayLast = datetime.strptime('20190414 2359', \"%Y%m%d %H%M\")\n",
    "\n",
    "# test_day1 = datetime.strptime('20190517 0600', \"%Y%m%d %H%M\")\n",
    "# test_day1_str = test_day1.strftime(\"%Y%m%d %H%M\")\n",
    "# dayLast = datetime.strptime('20190601 2359', \"%Y%m%d %H%M\")\n",
    "# dayLast_str = dayLast.strftime(\"%Y%m%d %H%M\")\n",
    "\n",
    "# val_day1 = datetime.strptime('20190602 0600', \"%Y%m%d %H%M\")\n",
    "# val_dayLast = val_day1 + timedelta(days=6) - timedelta(minutes=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo<br>\n",
    "model: 要訓練的神經網路模型<br>\n",
    "filename: 選擇vd真值的csv檔<br>\n",
    "備註: 此程式會將frame數分為兩個model去訓練，並分別向左右延伸，找到最佳的訓練與測試集<br>\n",
    "frame_num : 取50frame 論文第10頁<br>\n",
    "frame_index.txt欄位說明: [CCTV,peak1 train, peak1 test, peak2 train, peak2 test] *此檔案須根據pretrain_nn的結果來手動編輯<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Spd_Avg'\n",
    "frame_num = 50\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='auto',\n",
    "                               verbose=0, restore_best_weights=True)\n",
    "\n",
    "model_count = 0\n",
    "\n",
    "\"\"\"Models to train\"\"\"\n",
    "#models = [c3d(), c6g3(), c5g6(), c4g12(), c3g25(), lrcn()]\n",
    "models = [c5g6(), c4g12(), c3g25(), lrcn()]\n",
    "\n",
    "for name, model in models:\n",
    "    if not os.path.exists(\"plot/\" + name):\n",
    "        os.makedirs(\"plot/\" + name)\n",
    "    total_name = 'plot/' + name + '/log.txt'\n",
    "    open(total_name, 'w').close()\n",
    "    for Id, (cctv_Id, vd_Id) in enumerate(zip(cctvs, vds)):\n",
    "        print(cctv_Id)\n",
    "        print(cctv_Id, file=open(total_name, 'a'))\n",
    "        if not os.path.exists(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1\"):\n",
    "            os.makedirs(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1\")\n",
    "            os.makedirs(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_2\")\n",
    "        else:\n",
    "            for f in os.listdir(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1\"):\n",
    "                os.remove(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_1/\" + f)\n",
    "            for f in os.listdir(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_2\"):\n",
    "                os.remove(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_2/\" + f)\n",
    "        vdDir = 'D:/vd/' + vd_Id + '/vd_speed/'\n",
    "        cctvDir = 'D:/wcs/cctv/' + cctv_Id + \"/\"\n",
    "        filename = vdDir + 'vdspd_20190728_20190729.csv'\n",
    "        total_frames = cctvDir + \"frame_num.txt\"\n",
    "        vd_df = pd.read_csv(filename, infer_datetime_format=True, index_col=0)\n",
    "        vd_df.index = pd.to_datetime(vd_df.index)\n",
    "        vd_df = vd_df.shift(-1)                              #往上位移一位 因為資料取得有延遲\n",
    "        vd_df.loc[vd_df.index[len(vd_df)-1]] = 90            #將位移完後的nan填補90\n",
    "        frame_df = pd.read_csv(total_frames, infer_datetime_format=True, index_col=0, header=None)\n",
    "        frame_df.index = pd.to_datetime(frame_df.index)      #必須要用to_datetime 因為loc才能找到 從2019/07/29 ---> 2019-07-29\n",
    "        total_test_index = []\n",
    "        FrameList = []\n",
    "        with open(cctvDir + \"frame_interval.txt\") as f:\n",
    "            content = f.readlines()\n",
    "        frame_interval = np.array([x.strip() for x in content], dtype=int)\n",
    "        for i in range(len(content)):\n",
    "            FrameList.append(FrameInterval())\n",
    "        print(datetime.now())\n",
    "        train_count = 0\n",
    "        test_count = 0\n",
    "        val_count = 0\n",
    "        day = day1                       #first day of training day\n",
    "        while day <= train_dayLast:\n",
    "            # print(day)\n",
    "            if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\")\\\n",
    "                    or day not in vd_df.index:\n",
    "                day += timedelta(minutes=1)\n",
    "                continue\n",
    "            train_count += 1\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_train(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 0\", day)\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_train(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 1\", day)\n",
    "             #     \"D:/wcs/cctv/nfbCCTV-N5-S-21.048-M/20190729_1644 + \" 0\" or \" 1\" <- 有空白 , \"2019-07-29 16:44:00\"\n",
    "            day += timedelta(minutes=1)                                      #\"以方便後面做切割判斷\"\n",
    "\n",
    "        day = test_day1                 #first day of testing day\n",
    "        while day <= dayLast:\n",
    "            # print(day)\n",
    "            if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                    or day not in vd_df.index:\n",
    "                day += timedelta(minutes=1)\n",
    "                continue\n",
    "            test_count += 1\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_test(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 0\", day)\n",
    "            total_test_index.append(day)\n",
    "            day += timedelta(minutes=1)\n",
    "\n",
    "        day = val_day1                  #first day of validating day\n",
    "        while day <= val_dayLast:\n",
    "            # print(day)\n",
    "            if not os.path.exists(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \".npy\") \\\n",
    "                    or day not in vd_df.index:\n",
    "                day += timedelta(minutes=1)\n",
    "                continue\n",
    "            val_count += 1\n",
    "            FrameList[int(frame_df.loc[day]) // 10] \\\n",
    "                .add_val(cctvDir + day.strftime(\"%Y%m%d\") + \"_\" + day.strftime(\"%H%M\") + \" 0\", day)\n",
    "            day += timedelta(minutes=1)\n",
    "\n",
    "        print(\"Total Train:\", train_count, file=open(total_name, 'a'))\n",
    "        print(\"Total test:\", test_count, file=open(total_name, 'a'))\n",
    "        print(\"Total val:\", val_count, file=open(total_name, 'a'))\n",
    "        print(\"Frame Num:\", len(frame_df))\n",
    "\n",
    "        ModelTrain_1 = []\n",
    "        ModelTest_1 = []\n",
    "        ModelTrain_2 = []\n",
    "        ModelTest_2 = []\n",
    "        with open('frame_index.txt', 'r') as f:        #frame_index.txt\"是手動編集的\n",
    "            for line in f:\n",
    "                if cctv_Id in line:\n",
    "                    t1, t2, t3, t4 = line.split(\",\")[1:]\n",
    "                    for i in t1.split(\"/\"):\n",
    "                        ModelTrain_1.append(int(i))\n",
    "                    for i in t2.split(\"/\"):\n",
    "                        ModelTest_1.append(int(i))\n",
    "                    for i in t3.split(\"/\"):\n",
    "                        ModelTrain_2.append(int(i))\n",
    "                    for i in t4.split(\"/\"):\n",
    "                        ModelTest_2.append(int(i))\n",
    "\n",
    "        train = [ModelTrain_1, ModelTrain_2]\n",
    "        test = [ModelTest_1, ModelTest_2]\n",
    "        # train = [ModelTrain_2]\n",
    "        # test = [ModelTest_2]\n",
    "\n",
    "        total_test = []\n",
    "\n",
    "        for peaks in range(2):\n",
    "            print(\"\\n*Peak %d\" % (peaks+1), file=open(total_name, 'a'))\n",
    "            print(\"Frame Num:\", frame_num, file=open(total_name, 'a'))\n",
    "            print(\"Train List:\", train[peaks], file=open(total_name, 'a'))\n",
    "            print(\"Test List:\", test[peaks], file=open(total_name, 'a'))\n",
    "            my_model, predict = compare_mae(train[peaks], test[peaks], frame_num, \"peak_%d\" % (peaks+1), model)\n",
    "            my_model.save(\"plot/\" + name + \"/\" + cctv_Id + \"/peak_%d\" % (peaks+1) + \"/model.h5\")\n",
    "            total_test += test[peaks]\n",
    "            index = 0\n",
    "            for t in test[peaks]:\n",
    "                temp = predict[index:index+len(FrameList[t].test_df)].tolist()\n",
    "                while len(temp) < len(FrameList[t].test_df):\n",
    "                    temp.append(temp[-1])\n",
    "                index += len(FrameList[t].test_df)\n",
    "                FrameList[t].test_df.insert(loc=0, column='predict', value=temp)\n",
    "        predict_df = pd.DataFrame(columns=['value'])\n",
    "        for i in total_test_index:\n",
    "            for j in total_test:\n",
    "                if i in FrameList[j].test_df.index:\n",
    "                    predict_df.loc[i] = [FrameList[j].test_df.loc[i]['predict']]\n",
    "        df = pd.DataFrame(index=total_test_index)\n",
    "        predict_all = np.array(predict_df.loc[total_test_index, 'value'].values)\n",
    "        vd_all = np.array(vd_df.loc[total_test_index, feature].values)\n",
    "        df.insert(loc=0, column='target', value=vd_all)\n",
    "        df.insert(loc=0, column='predict', value=predict_df)\n",
    "        df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='ffill')\n",
    "        # Fill first predicted value if needed\n",
    "        df = df.replace(r'\\s+', np.nan, regex=True).fillna(method='bfill')\n",
    "        df.to_csv(\"plot/\" + name + \"/\" + cctv_Id + \"/total_result.csv\")\n",
    "        print(\"Total predict:\", len(predict_df), file=open(total_name, 'a'))\n",
    "        print(\"Total test:\", len(vd_all), file=open(total_name, 'a'))\n",
    "        y = np.array(df['target'].values)\n",
    "        p = np.array(df['predict'].values)\n",
    "        mae = abs(y - p).mean()\n",
    "        plt.figure(num=None, figsize=(50, 5))\n",
    "        std = abs(y - p).std()\n",
    "        plt.title('MAE: %.2f, σ: %.2f' % (mae, std), fontsize=30)\n",
    "        plt.tick_params(labelsize=30)\n",
    "        plt.plot(y, label='Target')\n",
    "        plt.plot(p, label='Predict')\n",
    "        plt.legend(loc=3, prop={'size': 20})\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        # plt.title(mae_2)\n",
    "        plt.savefig(\"plot/\" + name + \"/\" + cctv_Id + \"/total.png\")\n",
    "        plt.close('all')\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        print(\"Total Std: %.2f\" % std, file=open(total_name, 'a'))\n",
    "        print(\"Total MAE: %.2f\\n\" % mae, file=open(total_name, 'a'))\n",
    "        print(\"**********************\\n\", file=open(total_name, 'a'))\n",
    "    model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
